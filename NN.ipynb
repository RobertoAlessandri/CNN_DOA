{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMzrhBGXQqXEeo/F5K+/I5+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RobertoAlessandri/CNN_DOA/blob/main/NN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ],
      "metadata": {
        "id": "Uz_MfMLwZi8r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "\n",
        "# Helper libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(tf.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jTmOlRlsZDUc",
        "outputId": "a0417da6-e2d8-4956-d72f-07fdace89f45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load the Dataset"
      ],
      "metadata": {
        "id": "HIYaI9NMZqRs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#timit_rir = keras.datasets.fashion_mnist\n",
        "\n",
        "#(train_audio, train_labels), (test_audio, test_labels) = timit_rir.load_data()"
      ],
      "metadata": {
        "id": "7_sN4OC0Zvub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading the dataset returns 4 NumPy arrays:\n",
        "* train_audio, train_labels are the training set.\n",
        "* test_audio, test_labels are the test set.\n",
        "\n",
        "Audio have a 512 * 14 size with values from [ to ]. Labels are an array of integers, ranging from 0 to 11 (or 35). Each audio is mapped to a single direction, we have to define the class names:"
      ],
      "metadata": {
        "id": "gzlyNys6aMr5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class_names12 = ['0', '30', '60', '90', '120', '150', '180', '210', '240', '270', '300', '330']\n",
        "class_names36 = ['0', '10', '20', '30', '40', '50', '60', '70', '80', '90', '100', '110', '120', '130', '140', '150', '160', '170', '180', '190', '200', '210', '220', '230', '240', '250', '260', '270', '280', '290', '300', '310', '320', '330', '340', '350']\n"
      ],
      "metadata": {
        "id": "42KtD5GFasnO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Brief Data Exploration"
      ],
      "metadata": {
        "id": "mdNcz3pQcDSa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# training set shape:\n",
        "print('Training set audio dimension:',str(train_audio.shape))\n",
        "print('Training set label dimension:',str(train_labels.shape))\n",
        "\n",
        "# test set shape:\n",
        "print('Test set audio dimension:',str(test_audio.shape))\n",
        "print('Test set label dimension:',str(test_labels.shape))"
      ],
      "metadata": {
        "id": "oV3uaRzccHz_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Pre-Processing"
      ],
      "metadata": {
        "id": "fzFgHUWXcYAW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspect one audio\n",
        "plt.figure()\n",
        "plt.imshow(train_audio[5000])\n",
        "plt.colorbar()\n",
        "plt.grid(False)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OMyhuhoOcl54"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We normalize the data in a range of 0 to 1 before feeding the data to the Neural Network model (?)"
      ],
      "metadata": {
        "id": "GRsfvePScxOl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_audio = train_audio / train_audio.max()\n",
        "\n",
        "test_audio = test_audio / train_audio.max()"
      ],
      "metadata": {
        "id": "b-v29IPfc4L6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's show new range\n",
        "plt.figure()\n",
        "plt.imshow(train_audio[5000])\n",
        "plt.colorbar()\n",
        "plt.grid(False)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FAnRuwl4dDDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's show the first 25 audio\n",
        "plt.figure(figsize=(10,10))\n",
        "for i in range(25):\n",
        "    plt.subplot(5,5,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.imshow(train_audio[i], cmap=plt.cm.binary)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4KN9yidWdLyi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Network Architecture\n",
        "\n",
        "- Each convolutional layer uses 64 convolution kernels with the size of 3*3, to learn local correlations between local T-F regions.\n",
        "- BN layer is used after each convolutional layer to improve the stability of the network and speed up the convergence of the network.\n",
        "- The activation function of convolutional layers and fully connected layers is ReLU.\n",
        "- Between the convolutional layer and the fully connected layer and after each fully connected layer, a droput procedure with rate 0.5 is used to avoid overfitting.\n",
        "- size and number of convolutional kernels = . ? \n",
        "number of nodes in the fully connected layers = 512?.\n",
        "- input = SI features\n",
        "- The fully connected layer combines all the features extracted by the convolution layer to reduce the input 2D feature matrix to a 1D feature vector to facilitate the output layer for classification processing.\n",
        "- SoftMax function is used to perform clssification\n",
        "- The final source DOA is estimated by maximizing the posterior prbability \n",
        "- In the CNN training, the cross-entropy functon is used as the loss function\n",
        "- We employ the Adam as the optimizer\n",
        "- Initial learning rate is set to be 10^-3\n",
        "- Maximum number of epochs = 100\n",
        "- Early stopping with a patience of 10 epochs measured on the validation set is als used to prevent overfitting.\n",
        "\n"
      ],
      "metadata": {
        "id": "HlrI5GWSSS_J"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eC6m1W7mMvTO"
      },
      "outputs": [],
      "source": [
        "filters = 64\n",
        "kernel_size = (3,3)\n",
        "strides = (1,1)\n",
        "input_shape = (14, 511, 10)\n",
        "rate = 0.5\n",
        "K = 12 # Then we will test with K = 36\n",
        "\n",
        "model = keras.Sequential ([\n",
        "  # input layer (14 * 511 * 10) (convolutional layers + batch normalization (BN) w ReLU)                     \n",
        "  keras.layers.Conv2D(filters=filters, kernel_size=kernel_size, strides=strides, activation='relu', input_shape=input_shape, padding='valid', data_format = 'channels_last', use_bias = True, name='conv1'),\n",
        "  keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=False, scale=False, beta_initializer=\"zeros\", gamma_initializer=\"ones\", moving_mean_initializer=\"zeros\", moving_variance_initializer=\"ones\", beta_regularizer=None, gamma_regularizer=None, beta_constraint=None, gamma_constraint=None, name = 'bn1'),\n",
        "  # 2nd convolutional layers + batch normalization (BN) w ReLU\n",
        "  keras.layers.Conv2D(filters=filters, kernel_size=kernel_size, strides=strides, activation='relu', padding='valid', data_format = 'channels_last', use_bias = True, name='conv2'),\n",
        "  keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=False, scale=False, beta_initializer=\"zeros\", gamma_initializer=\"ones\", moving_mean_initializer=\"zeros\", moving_variance_initializer=\"ones\", beta_regularizer=None, gamma_regularizer=None, beta_constraint=None, gamma_constraint=None, name = 'bn2'),\n",
        "  # dropout procedure with rate 0.5\n",
        "  tf.keras.layers.Dropout(rate, noise_shape=None, seed=None, name = 'dn1'),\n",
        "  # 1st fully connected layer w ReLU & dropout procedure with rate 0.5\n",
        "  tf.keras.layers.Dense(512, activation = 'relu', name = 'fc1'),\n",
        "  tf.keras.layers.Dropout(rate, noise_shape=None, seed=None, name = 'dn2'),\n",
        "  # 2nd fully connected layer w ReLU & dropout procedure with rate 0.5\n",
        "  tf.keras.layers.Dense(512, activation = 'relu', name = 'fc2'),\n",
        "  tf.keras.layers.Dropout(rate, noise_shape=None, seed=None, name = 'dn3'),\n",
        "  # SoftMax\n",
        "  #tf.keras.layers.Softmax(output_shape = K)\n",
        "  tf.keras.layers.Dense(K, activation = 'softmax', name = 'output'),\n",
        "])\n",
        "\n",
        "# keras.layers.Flatten(name='flatten'),?\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's show the architecture of the model\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "yf5xlAotYDlc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0694c7f-5a71-47c4-ee57-469676bbd5f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1 (Conv2D)              (None, 12, 509, 64)       5824      \n",
            "                                                                 \n",
            " bn1 (BatchNormalization)    (None, 12, 509, 64)       128       \n",
            "                                                                 \n",
            " conv2 (Conv2D)              (None, 10, 507, 64)       36928     \n",
            "                                                                 \n",
            " bn2 (BatchNormalization)    (None, 10, 507, 64)       128       \n",
            "                                                                 \n",
            " dn1 (Dropout)               (None, 10, 507, 64)       0         \n",
            "                                                                 \n",
            " fc1 (Dense)                 (None, 10, 507, 512)      33280     \n",
            "                                                                 \n",
            " dn2 (Dropout)               (None, 10, 507, 512)      0         \n",
            "                                                                 \n",
            " fc2 (Dense)                 (None, 10, 507, 512)      262656    \n",
            "                                                                 \n",
            " dn3 (Dropout)               (None, 10, 507, 512)      0         \n",
            "                                                                 \n",
            " output (Dense)              (None, 10, 507, 12)       6156      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 345,100\n",
            "Trainable params: 344,844\n",
            "Non-trainable params: 256\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compiling the Model\n"
      ],
      "metadata": {
        "id": "hUbir7ZsISmL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 1e-3\n",
        "optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss='sparse_categorical_crossentropy'\n",
        ")"
      ],
      "metadata": {
        "id": "LRZMpfRoIQ3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the Model"
      ],
      "metadata": {
        "id": "wSPtkoEBJf0w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(train_audio, train_labels, epochs=100)"
      ],
      "metadata": {
        "id": "xRDrNZ9DJj69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating Accuracy"
      ],
      "metadata": {
        "id": "732eu5N9Jq-s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check how model performs on test dataset\n",
        "test_loss, test_acc = model.evaluate(test_audio,  test_labels, verbose=2)\n",
        "\n",
        "print('\\nTest accuracy:', test_acc)"
      ],
      "metadata": {
        "id": "b-XOccHAJvQP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predictions"
      ],
      "metadata": {
        "id": "Qc_BoIcTJyir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predictions over test set\n",
        "predictions = model.predict(test_audio)"
      ],
      "metadata": {
        "id": "WxRbuY8FJ2jq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show result\n",
        "audio_idx = 0 # Idx of image\n",
        "print('Model output:',predictions[audio_idx])\n",
        "print('Predicted label:', np.argmax(predictions[audio_idx]))\n",
        "print('Ground truth label:',test_labels[audio_idx])"
      ],
      "metadata": {
        "id": "4hZxNKV3J6oe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_image(i, predictions_array, true_label, img):\n",
        "    predictions_array, true_label, img = predictions_array, true_label[i], img[i]\n",
        "    plt.grid(False)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "\n",
        "    plt.imshow(img, cmap=plt.cm.binary)\n",
        "\n",
        "    predicted_label = np.argmax(predictions_array)\n",
        "    if predicted_label == true_label:\n",
        "        color = 'blue'\n",
        "    else:\n",
        "        color = 'red'\n",
        "\n",
        "    plt.xlabel(\"{} {:2.0f}% ({})\".format(class_names[predicted_label],\n",
        "                                100*np.max(predictions_array),\n",
        "                                class_names[true_label]),\n",
        "                                color=color)\n",
        "\n",
        "def plot_value_array(i, predictions_array, true_label):\n",
        "    predictions_array, true_label = predictions_array, true_label[i]\n",
        "    plt.grid(False)\n",
        "    plt.xticks(range(10))\n",
        "    plt.yticks([])\n",
        "    thisplot = plt.bar(range(10), predictions_array, color=\"#777777\")\n",
        "    plt.ylim([0, 1])\n",
        "    predicted_label = np.argmax(predictions_array)\n",
        "\n",
        "    thisplot[predicted_label].set_color('red')\n",
        "    thisplot[true_label].set_color('blue')"
      ],
      "metadata": {
        "id": "HthsE3agKEtq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_rows = 5\n",
        "num_cols = 3\n",
        "num_audio = num_rows*num_cols\n",
        "plt.figure(figsize=(2*2*num_cols, 2*num_rows))\n",
        "for i in range(num_audio):\n",
        "    plt.subplot(num_rows, 2*num_cols, 2*i+1)\n",
        "    plot_image(i, predictions[i], test_labels, test_audio)\n",
        "    plt.subplot(num_rows, 2*num_cols, 2*i+2)\n",
        "    plot_value_array(i, predictions[i], test_labels)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_dA3V_FIKQok"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}